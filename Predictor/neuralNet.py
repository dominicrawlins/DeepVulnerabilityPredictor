from multiprocessing import cpu_count
from typing import Union, NamedTuple
import torch
from torchvision.transforms import Compose
import torch.backends.cudnn
import numpy as np
from torch import nn, optim
from torch.nn import functional as F
import torchvision.datasets
from torch.optim.optimizer import Optimizer
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms
from FextractorPreprocessor import FextractorPreprocessor
from VDataset import VDataset
import time, sys, copy


class Shape(NamedTuple):
    width: int
    channels: int

class CNN(nn.Module):
    def __init__(self, width1: int, width2: int, channels: int, dropout: float, type: str):
        super().__init__()
        self.input_shape = Shape(width1, channels)
        self.dropout = nn.Dropout(p=dropout)
        self.normaliseConv1 = nn.BatchNorm1d(
            num_features=4,
        )


        self.conv1 = nn.Conv1d(
            in_channels=self.input_shape.channels,
            out_channels=4,
            padding=(1),
            kernel_size=(3),
            bias=False,
            stride=(1),
        )
        self.initialise_layer(self.conv1)

        if(not type == 'parallel'):
            self.fc1 = nn.Linear(width1*4, 2)
            self.initialise_layer(self.fc1)

        else:
            self.fc11 = nn.Linear(width1*4, 10)
            self.initialise_layer(self.fc11)

            self.fc12 = nn.Linear(width2*4, 10)
            self.initialise_layer(self.fc12)

        self.fc2 = nn.Linear(20, 2)
        self.initialise_layer(self.fc2)



    def forward(self, vector1: torch.Tensor, vector2: torch.Tensor) -> torch.Tensor:
        if(vector2.nelement() == 0):
            #print(vector1.shape)
            x = self.conv1(vector1)
            #print(x.shape)
            x = self.normaliseConv1(x)
            #print(x.shape)
            x = F.relu(x)
            #print(x.shape)
            x = torch.flatten(x, 1)
            #print(x.shape)


            x = (self.fc1(x))
            #x.softmax

            return x

        else:
            x1 = self.conv1(vector1)
            x2 = self.conv1(vector2)

            x1 = self.normaliseConv1(x1)
            x2 = self.normaliseConv1(x2)

            x1 = F.relu(x1)
            x2 = F.relu(x2)

            x1 = torch.flatten(x1, 1)
            x2 = torch.flatten(x2, 1)

            x1 = (self.fc11(x1))
            x2 = (self.fc12(x2))

            x = torch.cat((x1, x2), 1)

            x = self.fc2(x)

            return x


    @staticmethod
    def initialise_layer(layer):
        if hasattr(layer, "weight"):
            nn.init.kaiming_normal_(layer.weight)


class Trainer:
    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        criterion: nn.Module,
        optimizer: Optimizer,
        device: torch.device,
    ):
        self.model = model.to(device)
        self.device = device
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.criterion = criterion
        self.optimizer = optimizer
        self.intermediate_results = None

        self.step = 0

    def train(
        self,
        epochs: int,
        val_frequency: int,
        print_frequency: int = 20,
        start_epoch: int = 0
    ):
        self.model.train()
        for epoch in range(start_epoch, epochs):
            self.model.train()
            data_load_start_time = time.time()
            for batch1, batch2, labels in self.train_loader:
                batch1 = batch1.to(self.device)
                batch2 = batch2.to(self.device)
                labels = labels.to(self.device)

                data_load_end_time = time.time()

                logits = self.model.forward(batch1, batch2)

                loss = self.criterion(logits, labels)

                loss.backward()

                self.optimizer.step()

                self.optimizer.zero_grad()

                with torch.no_grad():
                    preds = logits.argmax(-1)
                    accuracy = compute_accuracy(labels, preds)

                data_load_time = data_load_end_time - data_load_start_time
                step_time = time.time() - data_load_end_time

                if ((self.step + 1) % print_frequency) == 0:
                    self.print_metrics(epoch, accuracy, loss, data_load_time, step_time)

                self.step += 1
                data_load_start_time = time.time()

            # self.summary_writer.add_scalar("epoch", epoch, self.step)
            if ((epoch + 1) % val_frequency) == 0:
                int_results = self.validate()
                # self.validate() will put the model in validation mode,
                # so we have to switch back to train mode afterwards
                self.model.train()

        return int_results

    def print_metrics(self, epoch, accuracy, loss, data_load_time, step_time):
        epoch_step = self.step % len(self.train_loader)
        print(
                f"epoch: [{epoch}], "
                f"step: [{epoch_step}/{len(self.train_loader)}], "
                f"batch loss: {loss:.5f}, "
                f"batch accuracy: {accuracy * 100:2.2f}, "
                f"data load time: "
                f"{data_load_time:.5f}, "
                f"step time: {step_time:.5f}"
        )



    def validate(self):
        print("\n\nvalidating\n\n")
        results = {"preds": [], "labels": []}
        total_loss = 0
        self.model.eval()


        # No need to track gradients for validation, we're not optimizing.
        with torch.no_grad():
            for batch1, batch2, labels in self.val_loader:
                batch1 = batch1.to(self.device)
                batch2 = batch2.to(self.device)
                labels = labels.to(self.device)
                logits = self.model(batch1, batch2)
                loss = self.criterion(logits, labels)
                total_loss += loss.item()
                preds = logits.cpu().numpy()
                preds = logits.argmax(dim=-1).cpu().numpy()
                results["preds"].extend(list(preds))
                results["labels"].extend(list(labels.cpu().numpy()))


            print_accuracy(results, total_loss, self.val_loader)

        return results


def print_accuracy(results, total_loss, val_loader):
    compute_class_accuracy(
        np.array(results["labels"]), np.array(results["preds"])
    )
    if(total_loss):
        average_loss = total_loss / len(val_loader)
        print(f"validation loss: {average_loss:.5f}")


def compute_accuracy(
    labels: Union[torch.Tensor, np.ndarray], preds: Union[torch.Tensor, np.ndarray]
) -> float:

    assert len(labels) == len(preds)
    return float((labels == preds).sum()) / len(labels)


def compute_class_accuracy(labels: Union[torch.Tensor, np.ndarray], preds: Union[torch.Tensor, np.ndarray]) -> float:
    classLabel = [0] * 2
    classPred = [0] * 2
    acc = [0] * 2
    typearr = [0] * 4
    types = ["tp", "fp", "tn", "fn"]
    nameArray = ["not vulnerable", "vulnerable"]

    for i in range(len(labels)):
        #print(labels[i], preds[i])
        if(labels[i] == preds[i]):
            classLabel[labels[i]] += 1
            if(labels[i] == 1):
                typearr[0] += 1
            else:
                typearr[2] += 1
        else:
            if(labels[i] == 1):
                typearr[3] += 1
            else:
                typearr[1] += 1
        classPred[labels[i]] += 1

    for i in range(2):
        if(classPred[i]):
            acc[i] = classLabel[i]/classPred[i]
        print(f"Class accuracy for {nameArray[i]}: {acc[i] * 100:2.2f}")

    for i in range(4):
        print(f"{typearr[i]}: {types[i]}")

    totalAcc = 0
    for accur in acc:
        totalAcc += accur
    print(f"Overall Accuracy: {totalAcc * 50:2.2f}")

    totalCases = sum(typearr)
    actualPositive = sum(list(filter(lambda x: x == 1, labels)))/totalCases
    actualNegative = 1 - actualPositive

    #speedup = acc[1] / ((typearr[0]+typearr[1]) / totalCases)
    #print("Speedup: ", speedup)

def run():

    if torch.cuda.is_available():
        DEVICE = torch.device("cuda")
    else:
        DEVICE = torch.device("cpu")

    fp = FextractorPreprocessor()
    trainn, testn = fp.get_imported_vectorised_data(sys.argv[1])
    trainDS = VDataset(trainn)
    testDS = VDataset(testn)


    train_loader = torch.utils.data.DataLoader(
        trainDS,
        batch_size=32, shuffle=True,
        num_workers=8, pin_memory=True)        #print(vector1.shape)


    val_loader = torch.utils.data.DataLoader(
        testDS,
        batch_size=32, shuffle=False,
        num_workers=8, pin_memory=True)

    width1 = trainDS.__getitem__(0)[0].shape[1]
    width2 = 0 if(trainDS.__getitem__(0)[1].nelement() == 0) else trainDS.__getitem__(0)[1].shape[1]
    model = CNN(width1=width1, width2=width2, channels=1, dropout=0.5, type=sys.argv[1])

    criterion = nn.CrossEntropyLoss(weight=torch.tensor([1., 15.]).to(DEVICE))

    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


    trainer = Trainer(
        model, train_loader, val_loader, criterion, optimizer, DEVICE
    )

    results = trainer.train(
        epochs=50,
        print_frequency=50,
        val_frequency=5,
    )

    return results


if __name__ == "__main__":
    run()
