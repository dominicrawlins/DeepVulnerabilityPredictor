import subprocess
import os
import csv
import sys
import numpy as np
import random
from scipy.sparse import csr_matrix
from sklearn.feature_extraction.text import CountVectorizer


class VFile():
    def __init__(self, X, y, file):
        self.X = list()
        self.addX(X)
        self.y = y
        self.file = file

    def addX(self, X):
        self.X.append(X)



class FextractorPreprocessor():
    def __init__(self):
        self.functions = ['bash', 'chown', 'cp', 'mv', 'nano']
        self.data = None
        self.csv_file = os.path.expanduser("~/fex_static_data.csv")
        csv.field_size_limit(sys.maxsize)
        self.vectorizer = CountVectorizer(lowercase=False)




        self.vulnerable_programs = set()


    def generate_data(self):
        os.chdir('/bin')
        list(map(lambda x : subprocess.run(['fextractor', '--static', '--out-file', self.csv_file, x]), self.functions))

    def add_vulnerable_programs(self):
        print("Getting vulnerable programs csv")
        vul_prog_reader = csv.reader(open(os.path.expanduser("vulnerable_programs.csv")), quotechar='|')
        for line in vul_prog_reader:
            self.vulnerable_programs.add(line[0])

    def import_data(self, analysis_type):
        features = dict()

        train = dict()
        test = dict()

        if(analysis_type == 'static'):
            print("Loading static features")
            csv_file = os.path.expanduser("static_features.csv")

        elif(analysis_type == 'dynamic'):
            print("Loading dynamic features")
            csv_file = os.path.expanduser("dynamic_features.csv")
        else:
            print("Specify analysis type")
            exit(0)

        csv_reader = csv.reader(open(csv_file), quotechar='|')
        csv_reader = list(csv_reader)

        self.add_vulnerable_programs()

        allx = []
        print("Going through csv reader")
        for line in csv_reader:
            name = line[0].split("\t")[0]
            label = 1 if(line[0].split("\t")[0] in self.vulnerable_programs) else 0
            x = line[0].split("\t")[2] if analysis_type == 'dynamic' else line[0].split("\t")[1]
            if(not name in features):
                features[name] = VFile(x, label, name)
            else:
                features[name].addX(x)


        programs = (list(features.items()))
        print("Gonna shuffle")
        random.shuffle(programs)
        split = int(len(programs) * 0.8)
        for i in range(split):
            train[i] = programs[i][1]
        for idx, i in enumerate(range(split, len(programs))):
            test[idx] = programs[i][1]





        print(len(train.items()))
        print(len(test.items()))

        train = create_individual_dict(train)
        test  = create_individual_dict(test)


        print(len(train.items()))
        print(len(test.items()))

        for item in train.items():
            allx.append(str(item[1].X))
        for item in test.items():
            allx.append(str(item[1].X))

        print("Gonna vectorize")
        self.fit_vectorizer(allx)


        for i in range(len(list(train.items()))):
            train[i].X = list(self.vectorizer.transform(train[i].X).todense())

        for i in range(len(list(test.items()))):
            test[i].X= list(self.vectorizer.transform(test[i].X).todense())

        return train, test



    def fit_vectorizer(self, X):
        self.vectorizer.fit(X)


    def generate_bow(self, X):
        return self.vectorizer.transform(self.X).todense()


    def get_imported_vectorised_data(self, type):
        if(type == 'static' or type == 'dynamic'):
            train, test = self.import_data(type)
            return train, test
        elif(type == 'concat' or type == 'parallel'):
            strain, stest = self.import_data('static')
            dtrain, dtest = self.import_data('dynamic')

            total_features = dict()

            for item in strain.items():
                total_features[item[1].file] = item[1]
            for item in stest.items():
                total_features[item[1].file] = item[1]

            for item in dtrain.items():
                key = item[1].file
                if(key in total_features):
                    if(type == 'concat'):
                        total_features[key].X[0] = np.concatenate((total_features[key].X[0], item[1].X[0]), 1)
                    else:
                        total_features[key].X.append(item[1].X[0])

            for item in dtest.items():
                key = item[1].file
                if(key in total_features):
                    if(type == 'concat'):
                        total_features[key].X[0] = np.concatenate((total_features[key].X[0], item[1].X[0]), 1)
                    else:
                        total_features[key].X.append(item[1].X[0])

            print(len(total_features.keys()))
            if(type == 'concat'):
                keys = list(filter(lambda x:total_features[x].X[0].shape[1] == 456, total_features.keys()))
            else:
                keys = list(filter(lambda x:len(total_features[x].X) > 1, total_features.keys()))
            train = dict()
            test = dict()
            split = int(len(keys) * 0.8)
            for i in range(split):
                train[i] = total_features[keys[i]]

            for idx, i in enumerate(range(split, len(keys))):
                test[idx] = total_features[keys[i]]

            print("shape: ", len(train.keys()), len(test.keys()))
            return train, test


        exit(0)



def generateDict(csvreader):
    featuresDict = dict()


def create_individual_dict(olddict):
    temp_dict = dict()
    dict_count  = 0
    for i in range(len(olddict.items())):
        for idx,xfeature in enumerate(olddict[i].X):
            if(idx < 1):
                temp_dict[dict_count] = VFile(xfeature, olddict[i].y, olddict[i].file)
                dict_count += 1

    return temp_dict
