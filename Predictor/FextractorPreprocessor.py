import subprocess
import os
import csv
import sys
import numpy as np
import random
from scipy.sparse import csr_matrix
from sklearn.feature_extraction.text import CountVectorizer


class VFile():
    def __init__(self, file):
        self.X = dict()
        self.file = file

    def addX(self, x, feature):
        self.X[feature] = x

    def add_label(self, label):
        self.label = label




class FextractorPreprocessor():
    def __init__(self):
        csv.field_size_limit(sys.maxsize)


    def add_vulnerable_programs(self, vulprog_file="vulnerable_programs.csv"):
        print("Getting vulnerable programs csv")
        vul_prog_reader = csv.reader(open(os.path.expanduser(vulprog_file)), quotechar='|')
        for line in vul_prog_reader:
            self.vulnerable_programs.add(line[0])


    def get_own_dynamic(self, dir):
        sdict = dict()
        all_files = set()

        vocab = set()
        for dfile in (os.listdir(os.path.expanduser(dir))):
            if("counts" in dfile):
                with open(os.path.expanduser(dir + "/" + dfile)) as file:
                    for line in list(filter(lambda x: x[0] != '#', file)):
                        vocab.add(line.split()[1].strip())

        vocab = list(vocab)
        bow_index_dict = dict()
        for i in range(len(vocab)):
            bow_index_dict[vocab[i]] = i

        for dfile in (os.listdir(os.path.expanduser(dir))):
            if("counts" in dfile):
                with open(os.path.expanduser(dir + "/" + dfile)) as file:
                    prog = dfile.split("_")[0]
                    bow_arr = np.zeros((len(vocab)*2, 1), dtype=int)
                    for line in list(filter(lambda x: x[0] != '#', file)):
                        bow_arr[bow_index_dict[line.split()[1].strip()]] = int(line.split()[2])
                        bow_arr[bow_index_dict[line.split()[1].strip()] + len(vocab)] = int(line.split()[3])
                    sdict[prog] = bow_arr
                    all_files.add(prog)

        self.update_total_programs(all_files)
        return sdict


    def get_own_static(self, dir):
        sdict = dict()
        all_files = set()

        intermediate_vocab = set()
        vocab = set()
        for dfile in (os.listdir(os.path.expanduser(dir))):
            if("angr" in dfile):
                with open(os.path.expanduser(dir + "/" + dfile)) as file:
                    for line in file:
                        word = line.split()[0].strip()
                        if(word in intermediate_vocab):
                            vocab.add(word)
                        else:
                            intermediate_vocab.add(word)


        vocabset = vocab
        vocab = list(vocab)
        bow_index_dict = dict()
        for i in range(len(vocab)):
            bow_index_dict[vocab[i]] = i

        for dfile in (os.listdir(os.path.expanduser(dir))):
            if("angr" in dfile):
                with open(os.path.expanduser(dir + "/" + dfile)) as file:
                    prog = dfile.split("_")[0]
                    bow_arr = np.zeros((len(vocab)*2, 1), dtype=int)
                    for line in file:
                        if(line.split()[0].strip() in vocabset):
                            bow_arr[bow_index_dict[line.split()[0].strip()]] = int(line.split()[1])
                            bow_arr[bow_index_dict[line.split()[0].strip()] + len(vocab)] = int(line.split()[2])
                    sdict[prog] = bow_arr
                    all_files.add(prog)

        self.update_total_programs(all_files)

        return sdict



    def update_total_programs(self, new_set):
        if(self.total_programs == set()):
            self.total_programs = new_set
        else:
            self.total_programs = self.total_programs.intersection(new_set)

    def get_bow(self, file):
        bow_dict = dict()
        all_files = set()
        with open(file) as ofile:
            content = ofile.readlines()
            for i in range(int(len(content)/2)):
                header = content[i*2].replace("#", "").strip()
                arr = np.reshape(np.fromstring(content[i*2+1], sep=','), (1,-1))
                bow_dict[header] = arr
                all_files.add(header)
        self.update_total_programs(all_files)
        return bow_dict

    def get_vectorised(self, file, vdiscover=False):
        vec_dict = dict()
        all_files = set()
        all_x = list()
        with open(file) as ofile:
            csvreader = csv.reader(ofile)
            for line in csvreader:
                name = line[0].split("\t")[0]
                if(not name in all_files):
                    x = line[0].split("\t")[2] if vdiscover else line[0].split("\t")[1]
                    vec_dict[name] = x
                    all_files.add(name)
                    all_x.append(str(x))

        self.vectorizer = CountVectorizer(lowercase=False)
        self.vectorizer.fit(all_x)
        for file in all_files:
            vec_dict[file] = self.vectorizer.transform([vec_dict[file]]).todense()
        self.update_total_programs(all_files)
        return vec_dict


    def get_dataset(self, vstatic, vdynamic, bows_inputted, to_concat, ostatic,  odynamic, vdiscovervdynamic=False):
        self.total_programs = set()
        self.vulnerable_programs = set()

        self.add_vulnerable_programs()

        self.allfeatures = dict()


        vstatic_dict = None
        vdynamic_dict = None
        ostatic_dict = None
        odynamic_dict = None

        if(vstatic and bows_inputted):
            vstatic_dict = (self.get_bow(vstatic))
        elif(vstatic):
            vstatic_dict = (self.get_vectorised(vstatic))
        if(vdynamic and bows_inputted):
            vdynamic_dict = (self.get_bow(vdynamic))
        elif(vdynamic and vdiscovervdynamic):
            vdynamic_dict = (self.get_vectorised(vdynamic, vdiscover=True))
        elif(vdynamic):
            vdynamic_dict = (self.get_vectorised(vdynamic))

        if(odynamic):
            odynamic_dict = self.get_own_dynamic(odynamic)

        if(ostatic):
            ostatic_dict = self.get_own_static(ostatic)

        temp_dataset = dict()
        for program in (self.total_programs):
            temp_dataset[program] = VFile(program)
            label = 1 if program in self.vulnerable_programs else 0
            temp_dataset[program].add_label(label)



        if(vstatic_dict and not to_concat):
            for program in self.total_programs:
                feature = vstatic_dict[program]
                temp_dataset[program].addX(feature, 'vstatic')
        if(vdynamic_dict and not to_concat):
            for program in self.total_programs:
                feature = vdynamic_dict[program]
                temp_dataset[program].addX(feature, 'vdynamic')

        if(to_concat):
            for program in self.total_programs:
                static_feature = vstatic_dict[program]
                dynamic_feature = vdynamic_dict[program]
                feature = np.concatenate((static_feature, dynamic_feature), 1)
                temp_dataset[program].addX(feature, 'vconcat')

        if(odynamic_dict):
            for program in self.total_programs:
                feature = odynamic_dict[program]
                temp_dataset[program].addX(feature, 'odynamic')
        if(ostatic_dict):
            for program in self.total_programs:
                feature = ostatic_dict[program]
                temp_dataset[program].addX(feature, 'ostatic')

        shuffled_programs = list(self.total_programs)
        random.shuffle(shuffled_programs)
        split = int(len(shuffled_programs) * 0.8)
        train_programs = shuffled_programs[:split]
        test_programs = shuffled_programs[split:]

        train_dataset = dict()
        test_dataset = dict()


        for idx, train_program in enumerate(train_programs):
            train_dataset[idx] = temp_dataset[train_program]

        for idx, test_program in enumerate(test_programs):
            test_dataset[idx] = temp_dataset[test_program]


        return train_dataset, test_dataset



fp = FextractorPreprocessor()
fp.get_own_static("~/DeepVulnerabilityPredictor/data/angr/")


def create_individual_dict(olddict):
    temp_dict = dict()
    dict_count  = 0
    for i in range(len(olddict.items())):
        for idx,xfeature in enumerate(olddict[i].X):
            if(idx < 1):
                temp_dict[dict_count] = VFile(xfeature, olddict[i].y, olddict[i].file)
                dict_count += 1

    return temp_dict
