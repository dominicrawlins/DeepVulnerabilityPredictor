import subprocess
import os
import csv
import sys
import numpy as np
import random
from scipy.sparse import csr_matrix
from sklearn.feature_extraction.text import CountVectorizer
from gensim.models import Word2Vec
import itertools
from glove import Corpus, Glove
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
import ast
import math


class VFile():
    def __init__(self, file):
        self.X = dict()
        self.file = file

    def addX(self, x, feature):
        self.X[feature] = x

    def add_label(self, label):
        self.label = label




class FextractorPreprocessor():
    def __init__(self):
        csv.field_size_limit(sys.maxsize)


    def add_vulnerable_programs(self, vulprog_file="vulnerable_programs.csv"):
        print("Getting vulnerable programs csv")
        vul_prog_reader = csv.reader(open(os.path.expanduser(vulprog_file)), quotechar='|')
        for line in vul_prog_reader:
            self.vulnerable_programs.add(line[0].split("/")[-1])


    def get_own_dynamic(self, dir):
        sdict = dict()
        all_files = set()
        vocab = set()
        for dfile in (os.listdir(os.path.expanduser(dir))):
            if(os.path.exists((os.path.join(os.path.expanduser(dir), dfile, "out_counts")))):
                with open(os.path.join(os.path.expanduser(dir), dfile, "out_counts")) as file:
                    for line in list(filter(lambda x: x[0] != '#', file)):
                        vocab.add(line.split()[1].strip())
                        vocab.add("predicated" + line.split()[1].strip())
            else:
                print("nah", dfile)



        max_val_dict = dict()
        min_val_dict = dict()

        for dfile in (os.listdir(os.path.expanduser(dir))):
            with open(os.path.join(os.path.expanduser(dir), dfile, "out_counts")) as file:
                prog = dfile.split("_")[0].split("-repor")[0]
                for line in list(filter(lambda x: x[0] != '#', file)):
                    ins = line.split()[1].strip()
                    nopred = int(line.split()[2])
                    pred = int(line.split()[3])

                    if(ins in max_val_dict):
                        if(nopred > max_val_dict[ins]):
                            max_val_dict[ins] = nopred
                        if(nopred < min_val_dict[ins]):
                            min_val_dict[ins] = nopred
                    else:
                        max_val_dict[ins] = nopred
                        min_val_dict[ins] = nopred
                    if("predicated"+ ins in max_val_dict):
                        if(pred > max_val_dict["predicated"+ ins]):
                            max_val_dict["predicated"+ ins] = pred
                        if(pred < min_val_dict["predicated"+ ins]):
                            min_val_dict["predicated"+ ins] = pred
                    else:
                        max_val_dict["predicated"+ ins] = pred
                        min_val_dict["predicated"+ ins] = pred

        for ins in max_val_dict.keys():
            if(max_val_dict[ins] == 0 and min_val_dict[ins] == 0):
                vocab.remove(ins)
            if(max_val_dict[ins] == min_val_dict[ins]):
                min_val_dict[ins] = 0

        vocab = list(vocab)
        bow_index_dict = dict()
        for i in range(len(vocab)):
            bow_index_dict[vocab[i]] = i

        for dfile in (os.listdir(os.path.expanduser(dir))):
            with open(os.path.join(os.path.expanduser(dir), dfile, "out_counts")) as file:
                prog = dfile.split("_")[0].split("-repor")[0]
                bow_arr = np.zeros((len(vocab)*2, 1), dtype=float)
                for line in list(filter(lambda x: x[0] != '#', file)):
                    ins = line.split()[1].strip()
                    if(ins in bow_index_dict):
                        # print(max_val_dict[ins])
                        # print(math.log(max_val_dict[ins] + 1))
                        # print(int(line.split()[2]))
                        # print(math.log(int(line.split()[2])+1))
                        # print(min_val_dict[ins])
                        # print(math.log(min_val_dict[ins] + 1))

                        ans = ((int(line.split()[2])) - (min_val_dict[ins]))/ ((max_val_dict[ins]) - (min_val_dict[ins]))
                        # print("ans:", ans)
                        # print(bow_index_dict[ins])
                        # print("\n")
                        bow_arr[bow_index_dict[ins]] = ans
                    if("predicated" + ins in bow_index_dict):
                        bow_arr[bow_index_dict["predicated" + line.split()[1].strip()]] = ((int(line.split()[3])) - (min_val_dict["predicated"+ins]))/ ((max_val_dict["predicated"+ins]) - (min_val_dict["predicated"+ins]))

                sdict[prog] = np.reshape(bow_arr, (1,-1))

                all_files.add(prog)
                #exit(0)


        self.update_total_programs(all_files)
        return sdict


    def get_own_angr_static(self, dir):
        sdict = dict()
        all_files = set()

        intermediate_vocab = set()
        vocab = set()
        for dfile in (os.listdir(os.path.expanduser(dir))):
            if("angr" in dfile):
                with open(os.path.expanduser(dir + "/" + dfile)) as file:
                    for line in file:
                        word = line.split()[0].strip()
                        if(word in intermediate_vocab):
                            vocab.add(word)
                        else:
                            intermediate_vocab.add(word)


        vocabset = vocab
        vocab = list(vocab)
        bow_index_dict = dict()
        for i in range(len(vocab)):
            bow_index_dict[vocab[i]] = i

        for dfile in (os.listdir(os.path.expanduser(dir))):
            if("angr" in dfile):
                with open(os.path.expanduser(dir + "/" + dfile)) as file:
                    prog = dfile.split("_")[0]
                    bow_arr = np.zeros((len(vocab)*2, 1), dtype=int)
                    for line in file:
                        if(line.split()[0].strip() in vocabset):
                            bow_arr[bow_index_dict[line.split()[0].strip()]] = int(line.split()[1])
                            bow_arr[bow_index_dict[line.split()[0].strip()] + len(vocab)] = int(line.split()[2])
                    sdict[prog] = np.reshape(bow_arr, (1,-1))
                    all_files.add(prog)

        self.update_total_programs(all_files)


        return sdict

    def get_own_static(self, dir):
        cfg_dict = dict()

        all_files = set()

        first_vocab = set()
        second_vocab=set()
        end_vocab = set()

        for idx, dfile in enumerate(os.listdir(os.path.expanduser(dir))):
            sys.stdout.write("\rGetting vocab %i" % idx)
            sys.stdout.flush()
            if("cfg" in dfile):
                with open(os.path.expanduser(dir + "/" + dfile)) as file:
                    file_vocab = set()
                    next(file)
                    next(file)

                    cfg = ast.literal_eval(next(file))
                    for edge in cfg:
                        func = edge[1].strip()
                        file_vocab.add(func)
                    end_vocab = end_vocab | (second_vocab & file_vocab)
                    second_vocab = second_vocab | (first_vocab & file_vocab)
                    first_vocab = first_vocab | file_vocab

        print("\n")


        vocab_dict = dict()

        for idx, word in enumerate(end_vocab):
            vocab_dict[word] = idx


        for idx, dfile in enumerate(os.listdir(os.path.expanduser(dir))):
            sys.stdout.write("\rPreprocessing %i" % idx)
            sys.stdout.flush()
            if("cfg" in dfile):
                with open(os.path.expanduser(dir + "/" + dfile)) as file:
                    prog = dfile.split("_")[0]
                    out_vector = np.zeros(((len(end_vocab)*2) + 3, 1), dtype=int)
                    edges = int(next(file).split(":")[1])
                    nodes = int(next(file).split(":")[1])

                    cfg = ast.literal_eval(next(file))
                    for edge in cfg:
                        func = edge[1].strip()
                        if(func in end_vocab):
                            out_vector[vocab_dict[func]] += 1
                        else:
                            out_vector[-3] += 1
                    if(out_vector[-2] > 0):
                        print("we have a problem")
                    out_vector[-2] = edges
                    out_vector[-1] = nodes
                    cfg_dict[prog] = np.reshape(out_vector, (1,-1))
                    all_files.add(prog)


        self.update_total_programs(all_files)


        return cfg_dict




    def update_total_programs(self, new_set):
        if(self.total_programs == set()):
            self.total_programs = new_set
        else:
            self.total_programs = self.total_programs.intersection(new_set)

    def get_bow(self, file):
        bow_dict = dict()
        all_files = set()
        with open(file) as ofile:
            content = ofile.readlines()
            for i in range(int(len(content)/2)):
                header = content[i*2].replace("#", "").strip().split("/")[-1]
                arr = np.reshape(np.fromstring(content[i*2+1], sep=','), (1,-1))
                bow_dict[header] = arr
                all_files.add(header)
        self.update_total_programs(all_files)
        return bow_dict

    def get_vectorised(self, file, vdiscover=False):
        vec_dict = dict()
        all_files = set()
        all_x = list()
        with open(file) as ofile:
            csvreader = csv.reader(ofile)
            for line in csvreader:
                name = line[0].split("\t")[0].split("/")[-1]
                if(not name in all_files):
                    x = line[0].split("\t")[2].split("/")[-1] if vdiscover else line[0].split("\t")[1]
                    vec_dict[name] = x
                    all_files.add(name)
                    all_x.append(str(x))

        self.vectorizer = CountVectorizer(lowercase=False)
        self.vectorizer.fit(all_x)
        for file in all_files:
            vec_dict[file] = self.vectorizer.transform([vec_dict[file]]).todense()
        self.update_total_programs(all_files)
        return vec_dict


    def get_dataset(self, vstatic, vdynamic, bows_inputted, to_concat, ostatic,  odynamic, word2vecvdynamic, word2vecvstatic, glovevdynamic, glovevstatic, doc2vecvdynamic, doc2vecvstatic, rnn, vdiscovervdynamic=False):
        self.total_programs = set()
        self.vulnerable_programs = set()

        self.add_vulnerable_programs()

        self.allfeatures = dict()


        vstatic_dict = None
        vdynamic_dict = None
        ostatic_dict = None
        odynamic_dict = None
        embedvdynamicdict = None
        embedvstaticdict = None

        if(word2vecvdynamic):
            embedvdynamicdict = self.word2vec(word2vecvdynamic, vdynamic=True)
        elif(glovevdynamic):
            embedvdynamicdict = self.glove(glovevdynamic, vdynamic=True)
        elif(doc2vecvdynamic):
            embedvdynamicdict = self.doc2vec(doc2vecvdynamic, vdynamic=True)
        elif(rnn):
            embedvdynamicdict = self.word2vec(rnn, vdynamic=True)
        if(word2vecvstatic):
            embedvstaticdict = self.word2vec(word2vecvstatic, vdynamic=False)
        elif(glovevstatic):
            embedvstaticdict = self.glove(glovevstatic, vdynamic=False)
        elif(doc2vecvstatic):
            embedvstaticdict = self.doc2vec(doc2vecvstatic, vdynamic=False)

        if(vstatic and bows_inputted):
            vstatic_dict = (self.get_bow(vstatic))
        elif(vstatic):
            vstatic_dict = (self.get_vectorised(vstatic))
        if(vdynamic and bows_inputted):
            vdynamic_dict = (self.get_bow(vdynamic))
        elif(vdynamic and vdiscovervdynamic):
            vdynamic_dict = (self.get_vectorised(vdynamic, vdiscover=True))
        elif(vdynamic):
            vdynamic_dict = (self.get_vectorised(vdynamic))

        if(odynamic):
            odynamic_dict = self.get_own_dynamic(odynamic)

        if(ostatic):
            ostatic_dict = self.get_own_static(ostatic)


        temp_dataset = dict()
        for program in (self.total_programs):
            temp_dataset[program] = VFile(program)
            label = 1 if program in self.vulnerable_programs else 0
            temp_dataset[program].add_label(label)



        if(vstatic_dict and not to_concat):
            for program in self.total_programs:
                feature = vstatic_dict[program]
                temp_dataset[program].addX(feature, 'vstatic')
        if(vdynamic_dict and not to_concat):
            for program in self.total_programs:
                feature = vdynamic_dict[program]
                temp_dataset[program].addX(feature, 'vdynamic')

        if(to_concat):
            for program in self.total_programs:
                static_feature = vstatic_dict[program]
                dynamic_feature = vdynamic_dict[program]
                feature = np.concatenate((static_feature, dynamic_feature), 1)
                temp_dataset[program].addX(feature, 'vconcat')

        if(odynamic_dict):
            for program in self.total_programs:
                feature = odynamic_dict[program]
                temp_dataset[program].addX(feature, 'odynamic')
        if(ostatic_dict):
            for program in self.total_programs:
                feature = ostatic_dict[program]
                temp_dataset[program].addX(feature, 'ostatic')

        if(embedvdynamicdict):
            featurename = None
            if(word2vecvdynamic):
                featurename = 'word2vecvdynamic'
            elif(glovevdynamic):
                featurename = 'glovevdynamic'
            elif(doc2vecvdynamic):
                featurename = 'doc2vecvdynamic'
            elif(rnn):
                featurename = 'rnn'
            for program in self.total_programs:
                feature = embedvdynamicdict[program]
                temp_dataset[program].addX(feature, featurename)

        if(embedvstaticdict):
            featurename = None
            if(word2vecvstatic):
                featurename = 'word2vecvstatic'
            elif(glovevstatic):
                featurename = 'glovevstatic'
            elif(doc2vecvstatic):
                featurename = 'doc2vecvstatic'
            for program in self.total_programs:
                feature = embedvstaticdict[program]
                temp_dataset[program].addX(feature, featurename)

        #shuffled_programs = list(self.total_programs)
        #random.shuffle(shuffled_programs)
        #split = int(len(self.total_programs) * 0.8)
        if(0):
            all_vulnerable_programs = (self.total_programs & self.vulnerable_programs)

            all_normal_programs = (list(self.total_programs - all_vulnerable_programs))
            random.shuffle(all_normal_programs)

            all_vulnerable_programs = (list(all_vulnerable_programs))
            random.shuffle(all_vulnerable_programs)


            imbalance = int(len(all_normal_programs) / len(all_vulnerable_programs))
            print("imbalance:", imbalance)


            vulnsplit = int(len(all_vulnerable_programs)*0.8)
            train_vuln = all_vulnerable_programs[:vulnsplit]
            test_vuln = all_vulnerable_programs[vulnsplit:]

            normsplit = int(len(all_normal_programs)*0.8)
            train_norm = all_normal_programs[:normsplit]
            test_norm = all_normal_programs[normsplit:]
            #train_programs = shuffled_programs[:split]
            #test_programs = shuffled_programs[split:]


            train_programs = (train_vuln + train_norm)
            random.shuffle(train_programs)
            test_programs = test_vuln + test_norm

            train_dataset = dict()
            test_dataset = dict()

            save_train = 'train_dataset_saved.txt'
            save_test = 'test_dataset_saved.txt'

            train_file = open(save_train, 'w')
            test_file = open(save_test, 'w')
            for idx, train_program in enumerate(train_programs):
                train_dataset[idx] = temp_dataset[train_program]
                train_file.write(train_program + "\n")
            train_file.close()

            for idx, test_program in enumerate(test_programs):
                test_dataset[idx] = temp_dataset[test_program]
                test_file.write(test_program + "\n")
            test_file.close()
        else:
            train_file = '69.89train.txt'
            test_file = '69.89test.txt'

            print("reusing")

            open_train_file =  open(train_file, 'r')
            open_test_file = open(test_file, 'r')

            train_programs = list(filter(lambda x: x in self.total_programs, list(map(lambda x: x.replace("\n", ""), list(open_train_file)))))
            test_programs = list(filter(lambda x: x in self.total_programs, list(map(lambda x: x.replace("\n", ""), list(open_test_file)))))

            train_dataset = dict()
            test_dataset = dict()
            for idx, train_program in enumerate(train_programs):
                train_dataset[idx] = temp_dataset[train_program]

            for idx, test_program in enumerate(test_programs):
                test_dataset[idx] = temp_dataset[test_program]



            open_train_file.close()
            open_test_file.close()



        return train_dataset, test_dataset

    def word2vec(self, location, vdynamic=False):
        vec_dict = dict()
        all_files = set()
        vocab = list()
        count = 1
        with open(location) as ofile:
            csvreader = csv.reader(ofile)
            for line in csvreader:
                name = line[0].split("\t")[0].split("/")[-1]
                if(not name in all_files):
                    x = line[0].split("\t")[2].split("/")[-1].split(" ")[:-1] if vdynamic else line[0].split("\t")[1].split(" ")[:-1]
                    if(len(x) > 1000):
                        x = x[-1000:]
                    vec_dict[name] = x
                    all_files.add(name)
                    vocab.append(x)
                    count += 1
        print("\n")
        print(len(vocab))
        model = Word2Vec(vocab, min_count=1, size=10)

        for afile in all_files:
            wordarray = np.zeros((1000, 10))
            x = vec_dict[afile]
            for idx, ins in enumerate(reversed(x)):
                word = model[ins]
                wordarray[999-idx, :] = word
            vec_dict[afile] = wordarray

        self.update_total_programs(all_files)
        return vec_dict



    #https://github.com/maciejkula/glove-python/issues/96

    def glove(self, location, vdynamic=False):
        vec_dict = dict()
        all_files = set()
        vocab = list()
        count = 1
        with open(location) as ofile:
            csvreader = csv.reader(ofile)
            for line in csvreader:
                name = line[0].split("\t")[0].split("/")[-1]
                if(not name in all_files):
                    x = line[0].split("\t")[2].split("/")[-1].split(" ")[:-1] if vdynamic else line[0].split("\t")[1].split(" ")[:-1]
                    if(len(x) > 1000):
                        x = x[-1000:]
                    vec_dict[name] = x
                    all_files.add(name)
                    vocab.append(x)
                    count += 1
        corpus = Corpus()
        corpus.fit(vocab, window=10)
        glove = Glove(no_components=10,learning_rate=0.05)
        glove.fit(corpus.matrix, epochs=4, no_threads=4, verbose=True)
        glove.add_dictionary(corpus.dictionary)




        for afile in all_files:
            wordarray = np.zeros((1000, 10))
            x = vec_dict[afile]
            for idx, ins in enumerate(reversed(x)):
                word = glove.word_vectors[glove.dictionary[ins]]
                wordarray[999-idx, :] = word
            vec_dict[afile] = wordarray


        self.update_total_programs(all_files)
        return vec_dict


    def doc2vec(self, location, vdynamic=False):
        vec_dict = dict()
        all_files = set()
        sentences = []
        count = 1
        with open(location) as ofile:
            csvreader = csv.reader(ofile)
            for line in csvreader:
                name = line[0].split("\t")[0].split("/")[-1]
                if(not name in all_files):
                    x = line[0].split("\t")[2].split("/")[-1].split(" ")[:-1] if vdynamic else line[0].split("\t")[1].split(" ")[:-1]
                    sentences.append(TaggedDocument(x, str(count)))
                    vec_dict[name] = x
                    all_files.add(name)
                    count += 1
        print("training doc2vec")
        model = Doc2Vec(sentences, vector_size=2, window=5,workers=4)


        for afile in all_files:
            x = vec_dict[afile]
            sentence = model.infer_vector(x)
            vec_dict[afile] = np.reshape((sentence), (1,-1))

        self.update_total_programs(all_files)
        return vec_dict




def create_individual_dict(olddict):
    temp_dict = dict()
    dict_count  = 0
    for i in range(len(olddict.items())):
        for idx,xfeature in enumerate(olddict[i].X):
            if(idx < 1):
                temp_dict[dict_count] = VFile(xfeature, olddict[i].y, olddict[i].file)
                dict_count += 1

    return temp_dict
