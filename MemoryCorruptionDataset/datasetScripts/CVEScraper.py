from bs4 import BeautifulSoup
import csv
import requests
import html
import re
from CSVManipulator import CSVManipulator

class CVEScraper:
    def __init__(self, hide_scraping=False):
        self.cveBaseUrl = "https://www.cvedetails.com/"
        #this will get all detailed memory corruptions
        self.cveDatabaseStartPage = "https://www.cvedetails.com/vulnerability-list/opmemc-1/memory-corruption.html"

        self.directoryUrls = []
        self.pageUrls = []

        self.csvFile = "memoryCorruptions.csv"

        self.hideScraping = hide_scraping
        self.csvm = CSVManipulator(self.csvFile, write_headers=True)

    def scrape(self):
        self.makeDirectoryUrls()
        self.makePageUrls()
        self.createVulnCSV()

    #get urls of all the pages detailing vulnerabilities
    def makeDirectoryUrls(self):
        page = requests.get(self.cveDatabaseStartPage)
        soup = BeautifulSoup(page.content, "html.parser")
        hyperNums = list(filter(lambda x: "Go to page" in str(x), soup.findAll('a', href=True)))
        self.directoryUrls += (list(map(lambda x: x['href'], hyperNums)))

    #get all urls of vulnerability pages
    def makePageUrls(self):
        for url in self.directoryUrls:
            print("Getting page ", int(re.findall(r'page=(\d*)&', url)[0]))
            page = requests.get(self.cveBaseUrl + url)
            soup = BeautifulSoup(page.content, "html.parser")
            vulnPages = list(filter(lambda x: "CVE-" in str(x), soup.findAll('a', href=True)))
            self.pageUrls += (list(map(lambda x: x['href'], vulnPages)))


    #make csv detailing all vulnerabilities
    def createVulnCSV(self):
        listLength = len(self.pageUrls)
        for idx, url in enumerate(self.pageUrls):
            page = requests.get(self.cveBaseUrl + url)
            soup = BeautifulSoup(page.content, "html.parser")

            details = str(soup.find(class_= "cvedetailssummary").text).strip().split("\n")[0]

            publishDate, updateDate = getDatesFromSoup(soup)

            name = url.split("/")[2]

            cvssScore, vulnTypes, cwdID = getVulnerabilitiesFromSoup(soup)

            products = getVulnProductsFromSoup(soup)

            if(not self.hideScraping):
                print("\n")
                print("name: ", name)
                print("details:", details)
                print("published date:", publishDate)
                print("updated date:", updateDate)
                print("cvss score: ", cvssScore)
                print("vulnerability types: ", vulnTypes)
                print("cwd ID:", cwdID)
                print("products:", products)

                print("\n")


            print("Writing to CSV vulnerability ", idx+1, " of ", listLength)

            self.csvm.writeCVEVulnerability(name=name, details=details, publish_date=publishDate,
                                            update_date=updateDate, cvss_score=cvssScore,
                                            vuln_types=vulnTypes, cwd_ID=cwdID, products=products)

def getVulnProductsFromSoup(soup):
    rawProductTable = soup.find('table', id="vulnversconuttable")
    products = ""
    if(rawProductTable):
        productTable = rawProductTable.findChildren(['tr'])[1:]
        for entry in productTable:
            refs = entry.find_all('a')
            oneProduct = ""
            for ref in reversed(refs):
                oneProduct += ref.text + "@"
            products += oneProduct[:-1] + "/"
        products = products[:-1]
    return products

def getVulnerabilitiesFromSoup(soup):
    scoreTable = soup.find('table', id="cvssscorestable").findChildren(['tr'])

    cvssScore = scoreTable[0].text.split("\n")[2]

    rawVulnText = scoreTable[7].find_all('span')
    vulnTypes = ""
    for text in rawVulnText:
        vulnTypes += text.text + "/"
    vulnTypes = vulnTypes[:-1]

    cwdID = scoreTable[8].text.split("\n")[2]

    return cvssScore, vulnTypes, cwdID

def getDatesFromSoup(soup):
    dates = re.findall(r': (\d\d\d\d-\d\d-\d\d)', (str(soup.find(class_= "datenote").text).strip()))
    publishDate = dates[0]
    updateDate = dates[1]
    return publishDate, updateDate
